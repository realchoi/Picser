#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æœ¬åœ°åŒ– lint å·¥å…·

æ”¹è¿›ç‚¹ï¼š
1. æœç´¢æ‰€æœ‰å­—ç¬¦ä¸²å­—é¢é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ç‰¹å®šæ¨¡å¼
2. æ”¯æŒé€šè¿‡å˜é‡ä¼ é€’çš„æœ¬åœ°åŒ–é”®
3. æ”¯æŒ localized() å‡½æ•°è°ƒç”¨
4. æ›´å‡†ç¡®åœ°æ£€æµ‹å®é™…ä½¿ç”¨çš„é”®
"""

import json
import os
import re
import sys

# åŒ¹é… Swift ä»£ç ä¸­æ‰€æœ‰çš„å­—ç¬¦ä¸²å­—é¢é‡ï¼ˆä¸åœ¨æ³¨é‡Šä¸­ï¼‰
RE_STRING_LITERAL = re.compile(r'"([^"\n\\]*(?:\\.[^"\n\\]*)*)"')

# æ³¨é‡Šæ¨¡å¼ï¼Œç”¨äºæ’é™¤æ³¨é‡Šä¸­çš„å­—ç¬¦ä¸²
RE_SINGLE_LINE_COMMENT = re.compile(r'//.*$', re.MULTILINE)
RE_MULTI_LINE_COMMENT = re.compile(r'/\*.*?\*/', re.DOTALL)

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
XCSTRINGS_PATH = os.path.join(ROOT, "Languages", "Localizable.xcstrings")
SOURCE_DIRS = [os.path.join(ROOT, "Picser"), os.path.join(ROOT, "Tests")]


def load_localization_keys():
  """åŠ è½½ Localizable.xcstrings ä¸­çš„æ‰€æœ‰é”®"""
  with open(XCSTRINGS_PATH, "r", encoding="utf-8") as handler:
    raw = json.load(handler)
  strings = raw.get("strings", {})
  return set(strings.keys())


def remove_comments(content):
  """ç§»é™¤ Swift ä»£ç ä¸­çš„æ³¨é‡Š"""
  # å…ˆç§»é™¤å¤šè¡Œæ³¨é‡Š
  content = RE_MULTI_LINE_COMMENT.sub('', content)
  # å†ç§»é™¤å•è¡Œæ³¨é‡Š
  content = RE_SINGLE_LINE_COMMENT.sub('', content)
  return content


def collect_string_literals():
  """æ”¶é›†ä»£ç ä¸­æ‰€æœ‰çš„å­—ç¬¦ä¸²å­—é¢é‡"""
  literals = set()
  for directory in SOURCE_DIRS:
    for root, _, files in os.walk(directory):
      for filename in files:
        if not filename.endswith(".swift"):
          continue
        path = os.path.join(root, filename)
        with open(path, "r", encoding="utf-8") as handler:
          content = handler.read()

        # ç§»é™¤æ³¨é‡Šï¼Œé¿å…è¯¯æŠ¥
        content = remove_comments(content)

        # æå–æ‰€æœ‰å­—ç¬¦ä¸²å­—é¢é‡
        for match in RE_STRING_LITERAL.finditer(content):
          literal = match.group(1)
          # ä¸éœ€è¦è§£ç è½¬ä¹‰å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
          literals.add(literal)

  return literals


def main():
  localized_keys = load_localization_keys()
  string_literals = collect_string_literals()

  # æ‰¾å‡ºä»£ç ä¸­ä½¿ç”¨çš„æœ¬åœ°åŒ–é”®ï¼ˆå­—ç¬¦ä¸²å­—é¢é‡ä¸æœ¬åœ°åŒ–é”®çš„äº¤é›†ï¼‰
  used_keys = string_literals & localized_keys

  # æ‰¾å‡ºä»£ç ä¸­å¼•ç”¨ä½†åœ¨ xcstrings ä¸­ç¼ºå¤±çš„é”®
  # ï¼ˆè¿™é‡Œæˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„å¯å‘å¼åˆ¤æ–­ï¼š
  #  1. åŒ…å«ä¸‹åˆ’çº¿çš„å­—ç¬¦ä¸²
  #  2. å…¨æ˜¯å°å†™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
  #  3. é•¿åº¦åœ¨ 5 åˆ° 80 ä¹‹é—´ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿çš„ä¸å¤ªå¯èƒ½æ˜¯æœ¬åœ°åŒ–é”®ï¼‰
  #  4. ä¸åŒ…å«ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦ï¼‰
  potential_keys = {
    s for s in string_literals
    if '_' in s
    and s.replace('_', '').replace('0', '').replace('1', '').replace('2', '').replace('3', '').replace('4', '').replace('5', '').replace('6', '').replace('7', '').replace('8', '').replace('9', '').isalpha()
    and s.islower()
    and 5 <= len(s) <= 80
    and ' ' not in s
  }
  missing = sorted(potential_keys - localized_keys)

  # æ‰¾å‡ºæœªä½¿ç”¨çš„æœ¬åœ°åŒ–é”®
  unused = sorted(localized_keys - used_keys)

  # ç»Ÿè®¡ä¿¡æ¯
  print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
  print(f"  - æœ¬åœ°åŒ–é”®æ€»æ•°: {len(localized_keys)}")
  print(f"  - ä»£ç ä¸­ä½¿ç”¨çš„é”®: {len(used_keys)}")
  print(f"  - æœªä½¿ç”¨çš„é”®: {len(unused)}")
  print()

  if not missing and not unused:
    print("âœ… æœ¬åœ°åŒ–æ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰é”®éƒ½å·²åŒæ­¥ã€‚")
    return 0

  if missing:
    print("âŒ ä»£ç ä¸­å¼•ç”¨ä½†åœ¨ Localizable.xcstrings ä¸­ç¼ºå¤±çš„é”®ï¼š")
    for key in missing:
      print(f"  - {key}")
    print()

  if unused:
    print("âš ï¸  åœ¨ Localizable.xcstrings ä¸­å®šä¹‰ä½†æœªä½¿ç”¨çš„é”®ï¼š")
    for key in unused:
      print(f"  - {key}")
    print()
    print("ğŸ’¡ æç¤ºï¼šè¿™äº›é”®å¯èƒ½é€šè¿‡å˜é‡é—´æ¥ä½¿ç”¨ï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤åå†åˆ é™¤ã€‚")

  return 1 if missing else 0


if __name__ == "__main__":
  sys.exit(main())
